---
title: "432 Class 17"
author: Thomas E. Love, Ph.D.
date: "2025-03-18"
format:
  revealjs: 
    theme: dark
    embed-resources: true
    self-contained: true
    slide-number: true
    footnotes-hover: true
    preview-links: auto
    date-format: iso
    logo: 432-2025-pic.png
    footer: "432 Class 17 | 2025-03-18 | <https://thomaselove.github.io/432-2025/>"
---

## Today's Agenda

**Regression Models for Count Outcomes**

- Modeling approaches illustrated in these slides
  - Poisson Regression & Zero-Inflated Poisson (ZIP)
  - Negative Binomial Regression & Zero-Inflated Negative Binomial (ZINB)
  
Chapters 24-26 of the Course Notes describe this material, as well as hurdle models (next class) and tobit regression, and some additional issues with certain types of count models.


## `countreg` and `topmodels` packages

To build rootograms to visualize the results of regression models on count outcomes, I have decided for the moment to continue to use the `countreg` and `topmodels` packages, which are currently available only on R-Forge. To install, type:

```{r}
#| echo: true
#| eval: false
install.packages("countreg", repos="http://R-Forge.R-project.org")
install.packages("topmodels", repos="http://R-Forge.R-project.org")
```

into the R Console within R Studio. 

## Today's R Setup

```{r}
#| echo: true
#| warning: false
#| message: false

knitr::opts_chunk$set(comment=NA)

library(janitor); library(gt); library(broom) 
library(mosaic); library(Hmisc); library(patchwork)
library(rsample); library(yardstick); library(here)
library(conflicted)      ## resolve conflicts
library(topmodels)       ## for rootograms
library(MASS)            ## for glm.nb to fit NB models
library(pscl)            ## for zero-inflated and hurdle fits
library(lmtest)          ## for Vuong test
library(easystats)
library(tidyverse)

conflicts_prefer(dplyr::select(), dplyr::filter(), base::max(), 
                 base::sum(), yardstick::rmse(), yardstick::mae(),
                 pscl::zeroinfl())

theme_set(theme_bw())
```

# An Overview

## GLMs for Count Outcomes

We want to build a generalized linear model to predict count data using one or more predictors.

Count data are non-negative integers (0, 1, 2, 3, ...)

- the number of COVID-19 hospitalizations in Ohio yesterday
- the number of mutations within a particular search grid
- days in the past 30 where your mental health was poor

We'll use the Poisson and Negative Binomial probability distributions.

## The Poisson Probability Distribution

The Poisson probability model describes the probability of a given number of events occurring in a fixed interval of time or space.

- If events occur with a constant mean rate, and independently of the time since the last event, the Poisson model is appropriate.
  - A Poisson model might fit poorly due to **overdispersion**, where the variance of Y is larger than we'd expect based on the mean of Y.

## Poisson regression

- Poisson regression assumes that the outcome Y follows a Poisson distribution, and that the logarithm of the expected value of Y (its mean) can be modeled by a linear combination of a set of predictors. 
  - A Poisson regression makes the strong assumption that the variance of Y is equal to its mean.

We will use `glm` to fit Poisson models, by using `family = "Poisson"`.

## Dealing with Overdispersion

A Poisson model might fit poorly due to **overdispersion**, where the variance of Y is larger than we'd expect based on the mean of Y.

- *Quasipoisson* models are available which estimate an overdispersion parameter, but we'll skip those for now.

Instead, we'll look at other ways (especially zero-inflation and the negative binomial models) to address overdispersion.

## Negative Binomial Regression

- Negative binomial regression is a generalization of Poisson regression which loosens the assumption that the variance of Y is equal to its mean, and thus produces models which fit a broader class of data.

We will demonstrate the use of `glm.nb()` from the `MASS` package to fit negative binomial regression models.

## Zero-inflated approaches

- Both the Poisson and Negative Binomial regression approaches may under-estimate the number of zeros compared to the data.
- To better match the zero counts, zero-inflated models fit:
  - a logistic regression to predict the extra zeros, along with
  - a Poisson or Negative Binomial model to predict the counts, including some zeros.

We'll use `zeroinfl()` from `pscl` to fit ZIP and ZINB regressions.

## Hurdle models

A hurdle model predicts the count outcome by making an assumption that there are two processes at work:

- a process that determines whether the count is zero or not zero (usually using logistic regression), and
- a process that determines the count when we know the subject has a positive count (usually using a truncated Poisson or NB model where no zeros are predicted)

We use `hurdle()` from `pscl` to fit these.

## Comparing Models

1. A key tool will be a graphical representation of the fit of the models to the count outcome, called a **rootogram**. We'll use the rootograms produced by the `countreg` and `topmodels` packages to help us.
2. We'll also demonstrate a Vuong hypothesis testing approach (from the `lmtest` package) to help us make decisions between various types of Poisson models or various types of Negative Binomial models on the basis of improvement in fit of things like bias-corrected AIC or BIC.

## Comparing Models

3. We'll also demonstrate the calculation of pseudo-R square statistics for comparing models, which can be compared in a validation sample as well as in the original modeling sample.

# The `medicare` data

## The `medicare` example

Source: `NMES1988` data in R's `AER` package, cleaned up to `medicare.csv`.

Essentially the same data are used in [\textcolor{blue}{my main resource}](http://data.library.virginia.edu/getting-started-with-hurdle-models/) from the University of Virginia on hurdle models.

Data are a cross-section US National Medical Expenditure Survey (NMES) conducted in 1987 and 1988. The NMES is based upon a representative, national probability sample of the civilian non-institutionalized population and individuals admitted to long-term care facilities during 1987. 

## Ingesting `medicare` data

The data are a subsample of individuals ages 66 and over all of whom are covered by Medicare (a public insurance program providing substantial protection against health-care costs), and some of whom also have private supplemental insurance.

```{r}
#| echo: true
medicare <- read_csv(here("c17/data/medicare.csv"), 
                     show_col_types = FALSE) |> 
  mutate(across(where(is_character), as_factor),
         subject = as.character(subject))
```

## The `medicare` code book {.smaller}

Variable | Description
---------: | --------------------------
`subject`  | subject number (code)
`visits`   | outcome: # of physician office visits
`hospital` | # of hospital stays
`health`   | self-rated health (poor, average, excellent)
`chronic`  | # of chronic conditions
`sex`      | male or female
`school`   | years of education
`insurance` | subject (also) has private insurance? (yes/no)

### Today's Goal

Predict `visits` using main effects of the 6 predictors (excluding `subject`)

## The `medicare` tibble

```{r}
#| echo: true
medicare |> select(-subject)
```

## Quick Summary of `medicare` {.smaller}

```{r}
#| echo: true
medicare |> select(-subject) |> summary()
```

### Adjust order of `insurance`

```{r}
#| echo: true
medicare <- medicare |>
  mutate(insurance = fct_relevel(insurance, "no", "yes"))
```

I want No first, then Yes, when building models.

## Our outcome, `visits`

```{r}
#| echo: true
favstats(~ visits, data = medicare)
describe(medicare$visits) # from Hmisc
```

```{r}
#| echo: true
#| output-location: slide
ggplot(medicare, aes(x = visits)) +
    geom_histogram(binwidth = 1, fill = "royalblue", 
                   col = "white") +
    labs(y = "Number of Patients", x = "Number of Visits")
```

## Partitioning the Data 

Creating Training and Testing Samples with `rsample` functions...

```{r}
#| echo: true
set.seed(432)
med_split <- initial_split(medicare, prop = 0.75)

med_train = training(med_split)
med_test = testing(med_split)
```

I've held out 25% of the `medicare` data for the test sample.

```{r}
#| echo: true
dim(med_train); dim(med_test)
```

## Reiterating the Goal {.smaller}

Predict `visits` using some combination of these 6 predictors...

Predictor | Description
---------: | ----------------------------------------------
`hospital` | # of hospital stays
`health`   | self-rated health (poor, average, excellent)
`chronic`  | # of chronic conditions
`sex`      | male or female
`school`   | years of education
`insurance` | subject (also) has private insurance? (yes/no)

We'll build separate training and test samples to help us validate.

# `mod_1`: A Poisson Regression

## Poisson Regression

Assume our count data (`visits`) follows a Poisson distribution with a mean conditional on our predictors.

```{r}
#| echo: true
mod_1 <- glm(visits ~ hospital + health + chronic +
                  sex + school + insurance,
              data = med_train, family = "poisson")
```

The Poisson model uses a logarithm as its link function, so the model is actually predicting log(`visits`).

Note that we're fitting the model here using the training sample alone.

## Complete `mod_1` Summary

```{r}
#| echo: true
summary(mod_1)
```


## `mod_1` (Poisson) model coefficients

```{r}
#| echo: true
tidy(mod_1) |> gt() |> fmt_number(decimals = 3)
```

Harry and Larry have the same values for all other predictors but only Harry has private insurance. `mod_1` estimates Harry's log(`visits`) to be `r tidy(mod_1) |> filter(term == "insuranceyes") |> select(estimate) |> round_half_up(digits = 3)` larger than Larry's log(`visits`).

## Dealing with log transformations

OK, you ran a regression/fit a linear model and some of your variables are log-transformed.

- We fit a linear model to predict the log of an outcome. How might we most effectively interpret the coefficients of that model? 
- How does our thinking change if we only take the log of a predictor?
- How about if we log both the outcome and the predictor?

Source [is here](https://library.virginia.edu/data/articles/interpreting-log-transformations-in-a-linear-model) for further reference (also see today's README.)

## Only the outcome is log-transformed {.smaller}

Exponentiate the coefficient. This gives the multiplicative factor for every one-unit increase in the independent variable. 

- Example: the coefficient is 0.198. exp(0.198) = 1.218962. For every one-unit increase in the independent variable, our dependent variable increases by a factor of about 1.22, or 22%. Recall that multiplying a number by 1.22 is the same as increasing the number by 22%. 
- Likewise, multiplying a number by, say 0.84, is the same as decreasing the number by 1 â€“ 0.84 = 0.16, or 16%.

from [this source](https://library.virginia.edu/data/articles/interpreting-log-transformations-in-a-linear-model)

## Only a predictor is log-transformed {.smaller}

- Divide the coefficient by 100. This tells us that a 1% increase in the independent variable increases (or decreases) the dependent variable by (coefficient/100) units. 

- Example: the coefficient is 0.198. 0.198/100 = 0.00198. For every 1% increase in the independent variable, our dependent variable increases by about 0.002. 

- For x percent increase, multiply the coefficient by log(1.x). 

- Example: For every 10% increase in the independent variable, our dependent variable increases by about 0.198 * log(1.10) = 0.02

from [this source](https://library.virginia.edu/data/articles/interpreting-log-transformations-in-a-linear-model)

## Both y and x are log-transformed {.smaller}

Interpret the coefficient as the percent increase in the dependent variable for every 1% increase in the independent variable. 

- Example: the coefficient is 0.198. For every 1% increase in the independent variable, our dependent variable increases by about 0.20%. 
- For x percent increase, calculate 1.x to the power of the coefficient, subtract 1, and multiply by 100. 
- Example: For every 20% increase in the independent variable, our dependent variable increases by about (1.20 0.198 - 1) * 100 = 3.7 percent.

from [this source](https://library.virginia.edu/data/articles/interpreting-log-transformations-in-a-linear-model)

## `mod_1` parameters

- Note the exponentiation here. IRR = incidence rate ratio

```{r}
#| echo: true

model_parameters(mod_1, exponentiate = TRUE, ci = 0.90)
```

- A change from no to yes in `insurance` (holding other predictors  constant) is associated with a 22% increase in our outcome, `visits`.

## `mod_1` performance summaries

```{r}
#| echo: true

model_performance(mod_1)
```

```{r}
#| echo: true

glance(mod_1)
```


## Visualize fit: (Hanging) Rootogram

```{r}
#| echo: true
plot(rootogram(mod_1, plot = FALSE), xlim = c(0, 90), 
               main = "Rootogram for mod_1: Poisson")
```

See the next slide for details on how to interpret this...

## Interpreting the Rootogram {.smaller}

- The red curved line is the theoretical Poisson fit. 
- "Hanging" from each point on the red line is a bar, the height of which represents the observed counts. 
    - A bar hanging below 0 indicates that the model under-predicts that value. (Model predicts fewer values than the data show.)
    - A bar hanging above 0 indicates over-prediction of that value. (Model predicts more values than the data show.)
- The counts have been transformed with a square root transformation to prevent smaller counts from getting obscured and overwhelmed by larger counts. 
- <https://arxiv.org/pdf/1605.01311> has more on rootograms.
- Our Poisson model (`mod_1`) doesn't fit enough zeros or ones, and fits too many 3-12 values, then not enough of the higher values.

## Checking `mod_1` (plots 1-2)

```{r}
#| echo: true
check_model(mod_1, check = c("pp_check", "overdispersion"))
```

## Checking `mod_1` (plots 3-4)

```{r}
#| echo: true
check_model(mod_1, check = c("homogeneity", "outliers"))
```

## Checking `mod_1` (plots 5-6)

```{r}
#| echo: true
check_model(mod_1, check = c("vif", "qq"))
```

## Store `mod_1` Predictions

We'll use the `augment` function to store the predictions within our training sample. Note the use of `"response"` to predict `visits`, not log(`visits`).

```{r}
#| echo: true
mod_1_aug <- augment(mod_1, med_train, 
                     type.predict = "response")

mod_1_aug |> select(subject, visits, .fitted) |> head(3)
```

## Training Sample `mod_1` Fit

Within our training sample, `mod_1_aug` now contains both the actual counts (`visits`) and the predicted counts (in `.fitted`) from `mod_1`. We'll summarize the fit...

```{r}
#| echo: true
mets <- metric_set(rsq, rmse, mae)
mod_1_summary <- 
  mets(mod_1_aug, truth = visits, estimate = .fitted) |>
  mutate(model = "mod_1") |> relocate(model)
mod_1_summary |> gt() |> fmt_number(decimals = 3)
```

These will become interesting as we build additional models.



# `mod_2`: A Negative Binomial Regression

## Fitting the Negative Binomial Model

The negative binomial model requires the estimation of an additional parameter, called $\theta$ (theta). The default link for this generalized linear model is also a logarithm, like the Poisson.

```{r}
#| echo: true
mod_2 <- MASS::glm.nb(visits ~ hospital + health + chronic +
                  sex + school + insurance,
              data = med_train)
```

The estimated dispersion parameter value $\theta$ is...

```{r}
#| echo: true
summary(mod_2)$theta
```

The Poisson model is essentially the negative binomial model assuming a known $\theta = 1$.

## Complete `mod_2` summary

```{r}
#| echo: true
summary(mod_2)
```


## `mod_2` (NB) coefficients

```{r}
#| echo: true
tidy(mod_2) |> gt() |> fmt_number(decimals = 3)
```

## `mod_2` parameters

```{r}
#| echo: true

model_parameters(mod_2, exponentiate = TRUE, ci = 0.90)
```


## `mod_2` performance summaries

```{r}
#| echo: true

model_performance(mod_2)
```

```{r}
#| echo: true

glance(mod_2)
```

## Rootogram for NB Model

```{r}
#| echo: true
plot(rootogram(mod_2, plot = FALSE), xlim = c(0, 90), 
               main = "Rootogram for mod_2: Negative Binomial")
```

Does this look better than the Poisson rootogram?

## Checking `mod_2` (plots 1-2)

```{r}
#| echo: true
check_model(mod_2, check = c("pp_check", "overdispersion"))
```

## Checking `mod_2` (plots 3-4)

```{r}
#| echo: true
check_model(mod_2, check = c("homogeneity", "outliers"))
```

## Checking `mod_2` (plots 5-6)

```{r}
#| echo: true
check_model(mod_2, check = c("vif", "qq"))
```

## Store `mod_2` Predictions

```{r}
#| echo: true
#| warning: false
mod_2_aug <- augment(mod_2, med_train, type.predict = "response")

mod_2_aug |> select(subject, visits, .fitted) |> head(3)
```

- Note that this *may* throw a warning about who maintains tidiers for `negbin` models. I'd silence it, as I have here.

## Training Fit for `mod_2`

`mod_2_aug` has actual (`visits`) and predicted counts (in `.fitted`.)

```{r}
#| echo: true
mod_2_summary <- 
  mets(mod_2_aug, truth = visits, estimate = .fitted) |>
  mutate(model = "mod_2") |> relocate(model)
mod_2_summary |> gt() |> fmt_number(decimals = 3)
```

## Training Sample So Far

The reasonable things to summarize in sample look like the impressions from the rootograms and the summaries we've prepared so far.

Model | Rootogram impressions
-----: | -------------------------------------------
`mod_1` (P) | Many problems. Data appear overdispersed.
`mod_2` (NB) | Still not enough zeros; some big predictions.

## Training Sample Summaries

```{r}
#| echo: true
bind_rows(mod_1_summary, mod_2_summary) |> 
  pivot_wider(names_from = model, 
              values_from = .estimate) |> 
  gt() |> fmt_number(decimals = 3) |> 
  tab_options(table.font.size = 20)
```

# `mod_3`: Zero-Inflated Poisson (ZIP) Model

## Zero-Inflated Poisson (ZIP) model

The zero-inflated Poisson model describes count data with an excess of zero counts. 

The model posits that there are two processes involved:

- a logistic regression model is used to predict excess zeros
- while a Poisson model is used to predict the counts

We'll use the `pscl` package to fit zero-inflated models.

```{r}
#| echo: true
mod_3 <- pscl::zeroinfl(visits ~ hospital + health + 
                    chronic + sex + school + insurance,
                    data = med_train)
```

## `mod_3` ZIP coefficients

Sadly, there's no `broom` tidying functions for these zero-inflated models.

```{r}
#| echo: true
summary(mod_3)
```

## `mod_3` parameters

```{r}
#| echo: true

model_parameters(mod_3, ci = 0.90)
```


## `mod_3` performance summaries

```{r}
#| echo: true

model_performance(mod_3)
```

- No `glance()` available.

## Rootogram for ZIP model

```{r}
#| echo: true
plot(rootogram(mod_3, plot = FALSE), xlim = c(0, 90), 
               main = "Rootogram for mod_3: ZIP")
```

## Check `mod_3` model

```{r}
check_model(mod_3, check = c("vif", "overdispersion"),
            residual_type = "normal", detrend = FALSE)
```
## Store `mod_3` Predictions

We have no `augment` or other `broom` functions available for zero-inflated models, so ...

```{r}
#| echo: true
mod_3_aug <- med_train |>
    mutate(".fitted" = predict(mod_3, type = "response"),
           ".resid" = resid(mod_3, type = "response"))

mod_3_aug |> select(subject, visits, .fitted, .resid) |>
  head(3)
```

## Training: `mod_3` Fit

`mod_3_aug` now has actual (`visits`) and predicted counts (in `.fitted`) from `mod_3`, just as we set up for the previous two models. 

```{r}
#| echo: true
mod_3_summary <- 
  mets(mod_3_aug, truth = visits, estimate = .fitted) |>
  mutate(model = "mod_3") |> relocate(model)
mod_3_summary |> gt() |> fmt_number(decimals = 3)
```

## Training: Through `mod_3`

```{r}
#| echo: true
bind_rows(mod_1_summary, mod_2_summary, mod_3_summary) |> 
  pivot_wider(names_from = model, 
              values_from = .estimate) |> 
  gt() |> fmt_number(decimals = 3) |> 
  tab_options(table.font.size = 20)
```

Remember we want a larger $R^2$ and smaller values of RMSE and MAE.

## Comparing models with Vuong

Vuong's test compares predicted probabilities (for each count) in two non-nested models. How about Poisson vs. ZIP?

```{r}
#| echo: true
vuong(mod_1, mod_3)
```

The large negative z-statistic indicates `mod_3` (ZIP) fits better than `mod_1` (Poisson) in our training sample.

# `mod_4`: Zero-Inflated Negative Binomial (ZINB) Model

## Zero-Inflated Negative Binomial (ZINB) model

As in the ZIP, we assume there are two processes involved:

- a logistic regression model is used to predict excess zeros
- while a negative binomial model is used to predict the counts

We'll use the `pscl` package again and the `zeroinfl` function.

```{r}
#| echo: true
mod_4 <- zeroinfl(visits ~ hospital + health + chronic +
                  sex + school + insurance,
              dist = "negbin", data = med_train)
```

## `mod_4` summary

```{r}
#| echo: true
summary(mod_4)
```

## `mod_4` parameters

```{r}
#| echo: true

model_parameters(mod_4, ci = 0.90)
```


## `mod_4` performance summaries

```{r}
#| echo: true

model_performance(mod_4)
```

- No `glance()` available.

## Rootogram for ZINB model

```{r}
#| echo: true

plot(rootogram(mod_4, plot = FALSE), xlim = c(0, 90), 
               main = "Rootogram for mod_4: ZINB")
```

## Check `mod_4` model

```{r}
check_model(mod_4, check = c("vif", "overdispersion"),
            residual_type = "normal", detrend = FALSE)
```

## Store `mod_4` Predictions

Again, there is no `augment` or other `broom` functions available for zero-inflated models, so ...

```{r}
#| echo: true
mod_4_aug <- med_train |>
    mutate(".fitted" = predict(mod_4, type = "response"),
           ".resid" = resid(mod_4, type = "response"))

mod_4_aug |> select(subject, visits, .fitted, .resid) |>
  head(3)
```

## Training Sample `mod_4` Fit

`mod_4_aug` now has actual (`visits`) and predicted counts (in `.fitted`) from `mod_4`. 

```{r}
#| echo: true
mod_4_summary <- 
  mets(mod_4_aug, truth = visits, estimate = .fitted) |>
  mutate(model = "mod_4") |> relocate(model)
mod_4_summary |> gt() |> fmt_number(decimals = 3)
```

## Training Sample through `mod_4`

```{r}
#| echo: true
bind_rows(mod_1_summary, mod_2_summary, 
          mod_3_summary, mod_4_summary) |> 
  pivot_wider(names_from = model, 
              values_from = .estimate) |> 
  gt() |> fmt_number(decimals = 3) |> 
  tab_options(table.font.size = 20)
```

What do you think?

## Comparing models with Vuong

How about Negative Binomial vs. ZINB?

```{r}
#| echo: true
vuong(mod_4, mod_2)
```

The large positive z-statistics indicate `mod_4` (ZINB) fits better than `mod_2` (Negative Binomial) in our training sample.

# Validation in the Test Sample for our Four Models?

## Validation: Test Sample Predictions

Predict the `visit` counts for each subject in our test sample.

```{r}
#| echo: true
test_1 <- predict(mod_1, newdata = med_test,
                  type.predict = "response")
test_2 <- predict(mod_2, newdata = med_test,
                  type.predict = "response")
test_3 <- predict(mod_3, newdata = med_test,
                  type.predict = "response")
test_4 <- predict(mod_4, newdata = med_test,
                  type.predict = "response")
```

## Create a Tibble with Predictions

Combine the various predictions into a tibble with the original data.

```{r}
#| echo: true
test_res <- bind_cols(med_test, 
              pre_m1 = test_1, pre_m2 = test_2, 
              pre_m3 = test_3, pre_m4 = test_4)

names(test_res)
```

## Summarize fit in test sample for each model

```{r}
#| echo: true
m1_sum <- mets(test_res, truth = visits, estimate = pre_m1) |>
  mutate(model = "mod_1") 
m2_sum <- mets(test_res, truth = visits, estimate = pre_m2) |>
  mutate(model = "mod_2") 
m3_sum <- mets(test_res, truth = visits, estimate = pre_m3) |>
  mutate(model = "mod_3")
m4_sum <- mets(test_res, truth = visits, estimate = pre_m4) |>
  mutate(model = "mod_4")

test_sum <- bind_rows(m1_sum, m2_sum, m3_sum, m4_sum)
```

## Validation Results: Four Models

```{r}
#| echo: true
test_sum <- bind_rows(m1_sum, m2_sum, m3_sum, m4_sum) |>
  pivot_wider(names_from = model, 
              values_from = .estimate)

test_sum |>
  select(-.estimator) |> 
  gt() |> fmt_number(decimals = 3) |> 
  tab_options(table.font.size = 20)
```

- Which model looks best? Is it an obvious choice?

